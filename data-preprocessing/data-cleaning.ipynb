{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vietnamese_alphabet = [\n",
    "    'a', 'à', 'á', 'ả', 'ã', 'ạ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ',\n",
    "    'b', 'c', 'd', 'đ', 'e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ',\n",
    "    'f', 'g', 'h', 'i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'k', 'l', 'm', 'n', 'o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ',\n",
    "    'p', 'q', 'r', 's', 't', 'u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự',\n",
    "    'v', 'w', 'x', 'y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'z'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_removal(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    pattern = r\"[^{vi_alpha}\\d=<>\\\"\\':/%$+&,.() ]\".format(vi_alpha = \"\".join(vietnamese_alphabet))\n",
    "    result = re.sub(pattern, '', text)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_normalization(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    pattern = r\"[\\-=<>\\\"\\':/%$+&,.()]\"\n",
    "    matches = re.finditer(pattern, text) \n",
    "    text = list(text)\n",
    "    \n",
    "    for match in matches:\n",
    "        text[match.start()] = ' ' + text[match.start()] + ' '\n",
    "    return \"\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def space_normalization(text: str) -> str:\n",
    "    pattern = r\"[ ]+\"\n",
    "    result = re.sub(pattern,' ',text)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'diner-reviews.txt'\n",
    "with open(fileName,'r') as readFile:\n",
    "    reviewList = [ast.literal_eval(ele.strip()) for ele in readFile.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(text: str) -> str:\n",
    "    text = emoji_removal(text)\n",
    "    text = punctuation_normalization(text)\n",
    "    text = space_normalization(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bánh canh cua ngon & giá ổn mình thấy review ở đây có nồi bánh canh cua là lạ nên lặn lội xơi ha để đi thử quán nằm trong hẻm , mà theo map cũng dễ kiếm lắm à nồi bánh canh hoành tráng lắm . topping ngập mặt , toàn món chất lượng . tôm to , chả cá , trứng cút , thịt nêm nếm rất ổn một nồi 92k có thể ăn 2 người ( no căng ) hoặc 3 người nếu ăn ít chút . gọi thêm đĩa quẩy 10k mình ghé tầm 5h chiều ngày thường , quán đông . phục vụ nhanh gọn team mê bánh canh cua là phải thử nha . xa nhà quá , chứ ko mình ghé hoài '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalization(\"Bánh canh cua ngon & giá ổn \\nMình thấy review ở đây có ‘nồi bánh canh cua’ là lạ nên lặn lội xơi ha để đi thử 😉\\nQuán nằm trong hẻm, mà theo map cũng dễ kiếm lắm à 🏘\\nNồi bánh canh hoành tráng lắm. Topping ngập mặt, toàn món chất lượng. Tôm to, chả cá, trứng cút, thịt… nêm nếm rất ổn 🤤\\nMột nồi 92k có thể ăn 2 người (no căng) hoặc 3 người nếu ăn ít chút. Gọi thêm đĩa quẩy 10k 🤑\\nMình ghé tầm 5h chiều ngày thường, quán đông. Phục vụ nhanh gọn 👍\\nTeam mê bánh canh cua là phải thử nha. Xa nhà quá, chứ ko mình ghé hoài 😊\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "sentiments = []\n",
    "for index,text in enumerate(reviewList):\n",
    "    sentiments.append(text[0])\n",
    "    texts.append(normalization(text[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveName = 'preprocessing/texts.txt'\n",
    "with open(saveName,'w') as writeFile:\n",
    "    for line in texts:\n",
    "        writeFile.write(str(line)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveName = 'preprocessing/sentiments.txt'\n",
    "with open(saveName,'w') as writeFile:\n",
    "    for line in sentiments:\n",
    "        writeFile.write(str(line)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
